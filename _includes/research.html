<p style="font-size:150%;text-align:center;color:rgba(0,0,0,.54)">Hi, please find my selected work-in-progress below!</p>
<div class="row g-5 mb-5">
  <div class="col-md-12">
    <h3 class="fw-bold border-bottom pb-3 mb-1">AI Fairness in Organizational Decisions: Conceptualization, Measurement, and Attainment from a Justice Theory Lens</h3>
      <p>(Under final preparation for submission to <i>MIS Quarterly</i>)</p>
    
      <p style="text-align:justify;">We develop a holistic theoretical framework to understand AI fairness in automating organizational decision tasks, along with a review of techniques for AI fairness assessment and attainment. We conceptualize AI fairness as two forms, i.e., outcome-based AI fairness and process-based AI fairness, and propose corresponding principles by which they are assessed. Embedded in the two forms of AI fairness, we develop a taxonomy for AI fairness assessment and attainment in organizational decision tasks. We employ the taxonomy to synthesize assessment measures and attainment approaches for AI fairness developed in the computer science and machine learning literature, bridging the organizational justice perspective with the technical perspective on AI fairness. Theoretically, our framework illuminates the interdependency and irreplaceability of AI fairness measures that differ in form, principles, and foci of fairness. For AI fairness attainment, the framework differentiates between reactive fairness attainment based on ex post adjustments and proactive fairness attainment based on ex ante constraints in the outputs or processes of the algorithm. Practically, our taxonomy has significant implications for responsible AI and AI fairness auditing. We illuminate that AI system owners and auditors need to decide on metrics of AI fairness based on the tradeoffs among the forms and principles of AI fairness and the levels of assessment for auditing goals for a task. The framework surfaces how reactive and proactive approaches can be selected based on interpretability and controllability preferences of auditors and system owners. </p>
    <p style="color:rgba(0,0,0,.54);">with Arun Rai, Ling Xue</p>
  </div>
</div>

<div class="row g-5 mb-5">
  <div class="col-md-12">
    <h3 class="fw-bold border-bottom pb-3 mb-3">Explainable AI and Human-AI Collaboration: A Control Perspective</h3>
    
    <p style="text-align:justify;">This paper is motivated to submit a theoretical framework that employs a control perspective to illuminate how to facilitate productive human-AI collaboration through explanations of AI. A control perspective to understand human-AI collaboration is necessary since the black-box and self-learning nature of AI makes it impossible to exhaustively expect all misbehaviors of AI. Distinguished from existing literature that emphasizes XAI as an enabler of human trust in AI, our study proposes that human trust in AI is not sufficient for, but a function of productive human-AI collaboration, the nature of which is effective human control over AI systems. In developing our framework, we theorize: (1) control needs of humans when working in joint with AI (2) explainability affordances emerging from functional features of XAI, and (3) the match between explainability affordances and control needs. In doing so, our framework bridges the gap between what organizations need and what the technical community is producing for the explanability of AI. Our study zooms in the collaborative task of human and AI, illuminating how explanations can enable humans to meaningfully reflect on algorithmic procedures and outputs and coordinate their actions efficiently. The framework generates implications for team restructuring to overcome the imperfection of technologies and achieve the complementarity between human and AI.</p>
    
    <p style="color:rgba(0,0,0,.54);">with Arun Rai</p>
  </div>
</div>

<div class="row g-5 mb-5">
  <div class="col-md-12">
    <h3 class="fw-bold border-bottom pb-3 mb-3">Strategic AI Innovation</h3>
    
    <p style="text-align:justify;">AI innovation is different from traditional IT innovations (e.g., software and semiconductors) by its core of machine learning algorithms, which are highly dependent on historical data and introduce both opportunities and risks for firms. To succeed with AI innovation, firms need a certain level of digitalization and the capability of generating and curating data from internal and external sources, which results in firms with higher IT capability gaining more advantages. Meanwhile, AI innovation requires effective integration of technical and domain expertise, highlighting the role of firms’ technological and governance structure in the outcome of AI innovation. This project delves into firms’ strategic choices regarding generativity and risks associated with AI innovation. Employing multisource archival data, the project differentiates types of AI innovation and examines their association with firms’ orientation, structure, and performance. This research is at the data analysis stage. </p>
    <p style="color:rgba(0,0,0,.54);">with Arun Rai</p>
  </div>
</div>

<div class="row g-5 mb-5">
  <div class="col-md-12">
    <h3 class="fw-bold border-bottom pb-3 mb-3">Strategic AI Innovation</h3>
    
    <p>AI innovation is different from traditional IT innovations (e.g., software and semiconductors) by its core of machine learning algorithms, which are highly dependent on historical data and introduce both opportunities and risks for firms. To succeed with AI innovation, firms need a certain level of digitalization and the capability of generating and curating data from internal and external sources, which results in firms with higher IT capability gaining more advantages. Meanwhile, AI innovation requires effective integration of technical and domain expertise, highlighting the role of firms’ technological and governance structure in the outcome of AI innovation. This project delves into firms’ strategic choices regarding generativity and risks associated with AI innovation. Employing multisource archival data, the project differentiates types of AI innovation and examines their association with firms’ orientation, structure, and performance. This research is at the data analysis stage.</p>
    <p style="color:rgba(0,0,0,.54);">with Arun Rai</p>
  </div>
</div>

<div class="row g-5 mb-5">
  <div class="col-md-12">
    <h3 class="fw-bold border-bottom pb-3 mb-1">Transparency or Perception Manipulation? A Study of IT Disclosure Tone in the Context of Data Breaches</h3>
    <p>(presented at <i>SCECR 2021</i>, <i>CSWIM 2021</i>, <i>CIST 2021</i>)</p>
    
    <p style="text-align:justify;">The paper examines peer firms’ strategy in disclosing their own IT when there are firms in their industries affected by data breaches and the role of IT governance. We leverage a neural network-based (NN-based) dependency parsing model to identify disclosure tones applied for specific IT-related information included in firms’ conference calls that incorporate comprehensive topics. We investigate how firms are motivated by the data breach events of other firms in the same industries to disclose their own IT-related information. We find that peer firms become more positive in tones to disclose their own IT-related information when other firms in their industries are affected by data breaches. We further examine the informativeness of the increased positivity in IT disclosure and how it is dependent on IT governance. We find that positive disclosure of IT information by firms with superior IT governance is mostly a transparent way to reveal strengthened IT protection. In contrast, firms with inferior IT governance are more likely to strategically use positive disclosure of IT information as a way to manipulate outsiders’ perception of IT robustness and safety.</p>
    <p style="color:rgba(0,0,0,.54);">with Ling Xue, Sean Cao, Yuan Long</p>
  </div>
</div>
